{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Text Generation using GPT-2\n",
        "\n",
        "# Step 1: Install the Transformers library (if not already installed)\n",
        "# Uncomment the line below if you're running this in a new environment\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "qbKLhrVMhUjC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "INvBcw8hhaRv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9OrfoajDhlCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Prepare the input prompt\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "Jbz9AxcAhiHY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate text\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYY5sDpQhry0",
        "outputId": "5bb38af6-a6e7-412a-9bc9-6a549e78577f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Decode and display the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZtIr-T4h0ft",
        "outputId": "2fab2d90-7fa5-4783-9832-9074db27a0dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "\n",
            "The future of Artificial Intelligence is in the hands of the next generation of AI.\n",
            "\n",
            "The next big thing is the future AI will be able to do. The next AI is going to be a machine that can do things like search, search for things, and so on. It's going be an AI that's not just a computer, but a human being. And that is what we're going for. We're not going away from that. That's what's happening. But we need\n"
          ]
        }
      ]
    }
  ]
}